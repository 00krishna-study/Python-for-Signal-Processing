{
 "metadata": {
  "name": "",
  "signature": "sha256:4e943fc715466ea027faabd1699df55f43dcb4328aad06e248ef940e731de35e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Confidence Intervals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a previous coin-flipping discussion, we discussed estimation of the underlying probability of getting a heads. There, we derived the estimator as \n",
      "\n",
      "$$ \\hat{p} = \\frac{1}{n}\\sum_{i=1}^n X_i $$\n",
      "\n",
      "Confidence intervals allow us to estimate how close we can get to the true value that we are estimating. Logically, that seems strange, doesn't it? We really don't know the exact value of what we are estimating (otherwise, why estimate it?), and yet, somehow we know how close we can get to something we admit we don't know? Ultimately, we want to make statements like the \"probability of the value in a  certain interval is 90%\". Unfortunately, that is something we will not be able to say using our methods. Note that Bayesian estimation gets closer to this statement by using \"credible intervals\", but that is a story for another day. In our situation, the best we can do is say roughly the following: \"if we ran the experiment multiple times, then the confidence interval would trap the true parameter 90% of the time\".\n",
      "\n",
      "Let's return to our coin-flipping example and see this in action. One way to get at a confidence interval is to use Hoeffding's inequality specialized to our Bernoulli variables as \n",
      "\n",
      "$$ \\mathbb{P}(|\\hat{p}-p|>\\epsilon) \\le 2 \\exp(-2n \\epsilon^2) $$ \n",
      "\n",
      "Now, we can form the interval $\\mathbb{I}=[\\hat{p}-\\epsilon_n,\\hat{p}+\\epsilon_n]$, where $\\epsilon_n$ is carefully constructed as\n",
      "\n",
      "$$ \\epsilon_n = \\sqrt{ \\frac{1}{2 n}\\log\\frac{2}{\\alpha}}$$\n",
      "\n",
      "which makes the right-side of the Hoeffding inequality equal to $\\alpha$. Thus, we finally have\n",
      "\n",
      "$$ \\mathbb{P}(p \\notin \\mathbb{I}) = \\mathbb{P}(|\\hat{p}-p|>\\epsilon_n) \\le \\alpha$$\n",
      "\n",
      "Thus, $ \\mathbb{P}(p \\in \\mathbb{I}) \\ge 1-\\alpha$. As a numerical example, let's take $n=100$, $\\alpha=0.05$, then plugging into everything we have gives $\\epsilon_n=0.136$. So, the 95% confidence interval here is therefore\n",
      "\n",
      "$$\\mathbb{I}=[\\hat{p}-\\epsilon_n,\\hat{p}+\\epsilon_n] = [\\hat{p}-0.136,\\hat{p}+0.136]$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import  bernoulli\n",
      "b=bernoulli(.5) # fair coin distribution\n",
      "xs = b.rvs(100*200).reshape(100,-1) # flip it 100 times for 200 estimates\n",
      "phat = mean(xs,axis=0) # estimated p\n",
      "epsilon_n= 0.136 # edge of 95% confidence interval\n",
      "print '--Interval trapped correct value ',np.logical_and(phat-epsilon_n<=0.5, 0.5 <= (epsilon_n +phat)).mean()*100,'% of the time'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "--Interval trapped correct value  99.0 % of the time\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result in the previous cell shows that the estimator and the corresponding interval was able to trap the true value at least 95% of the time. This is how to interpret the action of confidence intervals.\n",
      "\n",
      "However, the usual practice is to not use Hoeffding's inequality and instead use arguments around asymptotic normality. First, we need a concept of what *asymptotic* means."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Types of Convergence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Up to this point, we have been intuitively using some ideas about convergence that we now have to nail down. The first kind of convergence is *convergence in probability* which means the following:\n",
      "\n",
      "$$ \\mathbb{P}(|X_n -X|> \\epsilon) \\rightarrow 0$$\n",
      "\n",
      "as $n \\rightarrow 0$. This is notationally shown as $X_n  \\overset{P}{\\to} X$.\n",
      "\n",
      "The second major kind of convergence is *convergence in distribution* where\n",
      "\n",
      "$$ \\lim_{n \\to \\infty}  F_n(t) = F(t)$$\n",
      "\n",
      "for all $t$ for which $F$ is continuous. Bear in mind that we are talking about $F(t)$ as the probability cumulative density for $X$. This kind of convergence is usually annotated as $X_n \\rightsquigarrow X$. \n",
      "\n",
      "The third and final convergence we are interested in is *quadratic mean convergence* where\n",
      "\n",
      "$$ \\lim_{n\\rightarrow 0} \\mathbb{E}(X_n-X)^2 \\rightarrow 0 $$\n",
      "\n",
      "This is notationally shown as $X_n  \\overset{qm}{\\to} X$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Back to confidence intervals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The definition of the standard error is the following:\n",
      "\n",
      "$$ \\texttt{se} = \\sqrt{\\mathbb{V(\\hat{\\theta}_n)}}$$\n",
      "\n",
      "where $\\hat{\\theta}_n$ is the point-estimator for the parameter $\\theta$, given $n$ samples of data $X_n$. The $\\mathbb{V}$ is the variance of $\\hat{\\theta}_n$. Likewise, the estimated standard error is $\\hat{\\texttt{se}}$. For example, in our coin-flipping example, the estimator was $\\hat{p}=\\sum X_i/n$ with corresponding variance $\\mathbb{V}(\\hat{p}_n)=p(1-p)/n$\n",
      "\n",
      "we are plugging in a point estimate into a formula for the variance of $\\theta$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}