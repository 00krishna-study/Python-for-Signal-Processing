{
 "metadata": {
  "name": "Conditional_Expectation_Gaussian"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Zero-Mean Bivariate Gaussian Case"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Conditional expectation by Optimization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's consider the important case of the zero-mean bivariate Gaussian and try to find a  function $h$ that minimizes the MSE. Again,  trying to solve for the conditional expectation by minimizing the error over all possible functions $h$ is generally very, very hard. One alternative is to use parameters for the $h$ function and then just optimize over those. For example, we could assume that $h(Y)= \\alpha Y$ and then use calculus to find the $\\alpha$.\n",
      "\n",
      "Let's try this with the zero-mean bivariate Gaussian density,\n",
      "\n",
      "$$\\mathbb{E}((X-\\alpha Y )^2) = \\mathbb{E}(\\alpha^2 Y^2 - 2 \\alpha X Y + X^2 )$$\n",
      "\n",
      "and then differentiate this with respect to $\\alpha$ to obtain\n",
      "\n",
      "$$\\mathbb{E}(2 \\alpha Y^2 - 2 X Y  ) = 2 \\alpha \\sigma_y^2 - 2 \\mathbb{E}(XY) = 0$$\n",
      "\n",
      "Then, solving for $\\alpha$ gives\n",
      "\n",
      "$$ \\alpha = \\frac{ \\mathbb{E}(X Y)}{ \\sigma_y^2 } $$\n",
      "\n",
      "which means we that\n",
      "\n",
      "\\begin{equation}\n",
      "\\mathbb{ E}(X|Y) \\approx \\alpha Y =   \\frac{ \\mathbb{E}(X Y )}{ \\sigma_Y^2 } Y =\\frac{\\sigma_{X Y}}{ \\sigma_Y^2 } Y \n",
      "\\end{equation}\n",
      "\n",
      "where that last equality is just notation. Remember here we assumed a special linear form for $h=\\alpha Y$, but we did that for convenience. We still don't know whether or not this is the one true $h_{opt}$.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Conditional Expectation Using Direct Integration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's  compute $ \\mathbb{E}(X|Y)$ in the case of the bivariate Gaussian distribution straight from the definition.\n",
      "\n",
      "\\begin{equation}\n",
      "\\mathbb{E}(X|Y)  = \\int_{\\mathbb{ R}} x \\frac{f_{X,Y}(x,y)}{f_Y(y)} dx\n",
      "\\end{equation}\n",
      "\n",
      "where \n",
      "\n",
      "$$ f_{X,Y}(x,y) = \\frac{1}{2\\pi |\\mathbf{R}|^{\\frac{1}{2}}} e^{-\\frac{1}{2} \\mathbf{v}^T \\mathbf{R}^{-1} \\mathbf{v} } $$ \n",
      "\n",
      "and where\n",
      "\n",
      "$$ \\mathbf{v}= \\left[ x,y \\right]^T$$ \n",
      "\n",
      "$$ \\mathbf{R} = \\left[ \\begin{array}{cc}\n",
      "\\sigma_{x}^2 & \\sigma_{xy}  \\\\\\\\\n",
      "\\sigma_{xy}  & \\sigma_{y}^2 \\\\\\\\\n",
      "\\end{array} \\right] $$ \n",
      "\n",
      "and with\n",
      "\n",
      "\\begin{eqnarray}\n",
      " \\sigma_{xy} &=& \\mathbb{E}(xy)   \\nonumber    \\\\\\\\\n",
      " \\sigma_{x}^2 &=& \\mathbb{E}(x^2) \\nonumber    \\\\\\\\ \n",
      " \\sigma_{y}^2 &=& \\mathbb{E}(y^2) \\nonumber      \n",
      "\\end{eqnarray}\n",
      "\n",
      "This conditional expectation (Eq. 4 above) is a tough integral to evaluate, so we'll do it with `sympy`.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sympy import Matrix, Symbol, exp, pi, simplify, integrate \n",
      "from sympy import stats\n",
      "\n",
      "sigma_x = Symbol('sigma_x',positive=True)\n",
      "sigma_y = Symbol('sigma_y',positive=True)\n",
      "sigma_xy = Symbol('sigma_xy',real=True)\n",
      "fyy = stats.density(stats.Normal('y',0,sigma_y))(y)\n",
      " \n",
      "R = Matrix([[sigma_x**2, sigma_xy],\n",
      "            [sigma_xy,sigma_y**2]])\n",
      "fxy = 1/(2*pi)/sqrt(R.det()) * exp((-Matrix([[x,y]])*R.inv()* Matrix([[x],[y]]))[0,0]/2 )\n",
      "\n",
      "fcond = simplify(fxy/fyy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unfortunately, `sympy` cannot immediately integrate this without some hints. So, we need to define a positive variable ($u$) and substitute it into the integration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u=Symbol('u',positive=True) # define positive variable\n",
      "\n",
      "fcond2=fcond.subs(sigma_x**2*sigma_y**2-sigma_xy**2,u) # substitute as hint to integrate\n",
      "g=simplify(integrate(fcond2*x,(x,-oo,oo))) # evaluate integral\n",
      "gg=g.subs( u,sigma_x**2 *sigma_y**2 - sigma_xy**2 ) # substitute back in\n",
      "use( gg, simplify,level=2) # simplify exponent term"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "$$\\frac{\\sigma_{xy} y}{\\sigma_{y}^{2}}$$"
       ],
       "output_type": "pyout",
       "png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAApCAYAAAAmukmKAAAABHNCSVQICAgIfAhkiAAAAipJREFU\nWIXt109sTUEUx/FPX1sSQi2IIP5VUn9qY0HQSEmQ2BASCzZE7FhoSGz820iasCUhLC1F6EKaiqQ7\nFspKkDQhJQghJf5rWczc9LX1+t59va8W7Xczd2bOzO+8mXfPOZf/zBE8xHd8xRPcTLG+GgdwBY1x\nbCYeJAa5PONWzEEz5uEVlmNHCsGdaMNUrIhjm9Az1HAp7g9x4DHqUoiJ9nV4F0XhAloSg0RgM9rR\nH/uz8Bu9KQV7sQ2d+BLHNsb+IMH3+Ja38ByOpRRLmIvu+Lwk9h8lk9WxfYqtaBDurA0dcW6GcAK7\nsCw60iHc0T78Eo7wMm7hLfYIp3U49q+l8XhxbE9gtYF7XSjcz3zh2I7/Y20H9qcRg+moEbzPYVHe\n3I3Yno3ONOI5qtCELtTmb1atOKdQHxf+iGNvYrsOn3EUp+PYbCzAWhwU3udMacDFUo2rRiG0W3h9\npuCS8EsnGAckf5o/Yy1YDjkhuvSjD+cz8WgEtgtpDa4LcbYoueImBak3kJy7hQBQUSZjWnxuF9JQ\nUUqJpYXow09sEO7x9ij2Kpk6nBwLoYRDQhapFZJ0RdmLT0J58hErs9h0tHXqMGpGmGsVAkOzcGT3\nhDq1IlEpqzp1GIVe/Kzq1JIFs6xTB1EoeNfgDF4K3xmduJOFYDmsErLEGkzC1TSLywneH4Qy8DXW\n41mlBV8IgboHW3C30oKJaJOQE7vK3CM1qQrghHJKjIkCeJzzF8g2ahW/sT5UAAAAAElFTkSuQmCC\n",
       "prompt_number": 7,
       "text": [
        "sigma_xy\u22c5y\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "        2 \n",
        " sigma_y  "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus, by direct integration using `sympy`, we found\n",
      "\n",
      "$$ \\mathbb{ E}(X|Y) = y \\frac{\\sigma_{xy}}{\\sigma_{y}^{2}} $$ \n",
      "\n",
      "and this matches the prior result (Eq. 3) we obtained by assuming that $\\mathbb{E}(X|Y) = \\alpha Y$ and then solving for the optimal $\\alpha$. The importance of this result cannot be understated! It says that  $h_{opt}$ *is a linear function* of $Y$. In other words, assuming a linear function, which made the direct search for an optimal $h(Y)$ merely convenient yields the optimal result! This is  a general result that extends for *all* Gaussian problems. The link between linear functions and optimal estimation of Gaussian random variables is the most fundamental result in statistical signal processing! This fact is exploited in everything from optimal filter design  to adaptive signal processing.\n",
      "\n",
      "We can easily extend this result to non-zero mean problems as follows:\n",
      "\n",
      "$$ \\mathbb{ E}(X|Y) = \\bar{X} + (Y-\\bar{Y}) \\frac{\\sigma_{xy}}{\\sigma_{y}^{2}}  $$\n",
      "\n",
      "where $\\bar{X}$ is the mean of $X$ (same for $Y$)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Proof of MMSE of Conditional expectation by Optimization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ \\min_h \\int_\\mathbb{R^2} | X - h(Y) |^2 f_{x,y}(X,Y) dx dy $$\n",
      "\n",
      "$$ \\min_h \\int_\\mathbb{R^2} | X |^2 f_{x,y}(X,Y) dx dy + \\int_\\mathbb{R^2} | h(Y) |^2 f_{x,y}(X,Y) dx dy - \\int_\\mathbb{R^2} 2 X h(Y) f_{x,y}(X,Y) dx dy $$\n",
      "\n",
      "Now, we want to maximize the following:\n",
      "\n",
      "$$ \\max_h \\int_\\mathbb{R^2}  X h(Y) f_{x,y}(X,Y) dx dy  $$ \n",
      "\n",
      "Breaking up the integral using the definition of conditional expectation\n",
      "\n",
      "$$ \\max_h \\int_\\mathbb{R}   \\left(\\int_\\mathbb{R} X  f_{x|y}(X|Y) dx \\right)h(Y) f_Y(Y)   dy  $$ \n",
      "\n",
      "$$ \\max_h \\int_\\mathbb{R} \\mathbb{E}(X|Y) h(Y)f_Y(Y)   dy  $$ \n",
      "\n",
      "From properties of the Cauchy-Schwarz inequality, we know that the maximum happens when $h(Y) = \\mathbb{E}(X|Y)$, so we have found the optimal $h(Y)$ function as :\n",
      "\n",
      "$$ h(Y) = \\mathbb{E}(X|Y)$$ \n",
      "\n",
      "which shows that the optimal function is the conditional expectation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Constant as $\\mathbb{E}(X|Y)$ solution\n",
      "\n",
      "Suppose we have a random variable, $X$, then what constant is closest to $X$ in the mean-squared-sense (MSE)? In other words, which $c$ minimizes the following:\n",
      "\n",
      "$$ J = \\mathbb{E}( X - c )^2 $$ \n",
      "\n",
      "we can work this out as\n",
      "\n",
      "$$ \\mathbb{E}( X - c )^2 = \\mathbb{E}(c^2 - 2 c X + X^2)  $$ \n",
      "\n",
      "and then take the first derivative with respect to $c$ and solve:\n",
      "\n",
      "$$ c=\\mathbb{E}(X) $$ \n",
      "\n",
      "Remember that $X$ can take on all kinds of values, but this says that the closest number to $X$ in the MSE sense is $\\mathbb{E}(X)$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### References \n",
      "\n",
      "This post was created using the [nbconvert](https://github.com/ipython/nbconvert) utility from the source [IPython Notebook](www.ipython.org) which is available for [download](https://github.com/unpingco/Python-for-Signal-Processing/blob/master/Projection.ipynb) from the main github [site](https://github.com/unpingco/Python-for-Signal-Processing) for this blog. The projection concept is masterfully discussed in the classic Strang, G. (2003). *Introduction to linear algebra*. Wellesley Cambridge Pr. Also, some of Dr. Strang's excellent lectures are available on [MIT Courseware](http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/). I highly recommend these as well as the book."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Bibliography\n",
      "\n",
      "Nelson, Edward. Radically Elementary Probability Theory.(AM-117). Vol. 117. Princeton University Press, 1987.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}