{
 "metadata": {
  "name": "Conditional_Expectation_Projection"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "in our discussion, we previously noted that the conditional expectation is the minimum mean squared error (MMSE) solution to the following problem:\n",
      "\n",
      "$$ \\min_h \\int_{\\mathbb{R}} (x - h(y) )^2 dx $$ \n",
      "\n",
      "$$ \\mathbb{E}(X|Y) = h_{opt}(Y) $$\n",
      "\n",
      "which is another way of saying that among all possible functions $h(Y)$, the one that minimizes the MSE is $ \\mathbb{E}(X|Y)$. From our discussion on projection, we noted that these MMSE solutions can be thought of as projections onto a subspace that characterizes $Y$ in some sense. The amazing thing is that we can shift our notion of the inner product from our last discussion on projections and collect all of the same results (with some additional technicalities). For example, we previously noted that at the point of projection, we had\n",
      "\n",
      "$$ ( \\mathbf{y} - \\alpha \\mathbf{v} )^T \\mathbf{v} = 0$$\n",
      "\n",
      "which by noting the inner product as $\\langle\\mathbf{x},\\mathbf{y} \\rangle = \\mathbf{x}^T \\mathbf{y}$, we can express this as\n",
      "\n",
      "$$\\langle \\mathbf{y} - \\alpha \\mathbf{v},\\mathbf{v} \\rangle = 0  $$ \n",
      "\n",
      "and, in fact, by defining the inner product for the random variables $X$ and $Y$ as \n",
      "\n",
      "$$ \\langle X,Y \\rangle = \\mathbb{E}(X Y)$$ \n",
      "\n",
      "we have the same relationship,\n",
      "\n",
      "$$\\langle X-h_{opt}(Y),Y \\rangle = 0 =  \\mathbb{E}( X-\\mathbb{E}(X|Y), Y) $$ \n",
      "\n",
      "and using the linearity of the expectation, we can easily obtain,\n",
      "\n",
      "$$  \\mathbb{E}(X Y) =  \\mathbb{E}(Y \\mathbb{E}(X|Y))$$  \n",
      "\n",
      "If that seems too easy, we can proceed with a formal definition of conditional expectation\n",
      "\n",
      "$$ \\mathbb{E}(X|Y) = \\int_{\\mathbb{R}^2} x \\frac{f_{X,Y}(x,y)}{f_Y(y)} dx dy  $$\n",
      "\n",
      "and show this formally,\n",
      "\n",
      "$$ \\mathbb{E}(Y \\mathbb{E}(X|Y))= \\int_{\\mathbb{R}} y \\int_{\\mathbb{R}} x \\frac{f_{X,Y}(x,y)}{f_Y(y)}  f_Y(y) dx dy =\\int_{\\mathbb{R}^2} x y f_{X,Y}(x,y) dx dy =\\mathbb{E}( X Y) $$\n",
      "\n",
      "But doing it formally prohibits us from carrying along the geometric  interpretation from our previous discussion.\n",
      "\n",
      "We can keep pursuing this as before and obtain the length of the error term as\n",
      "\n",
      "$$ \\langle X-h_{opt}(Y),X-h_{opt}(Y)  \\rangle = \\langle X,X  \\rangle - \\langle h_{opt}(Y),h_{opt}(Y)  \\rangle  $$\n",
      "\n",
      "and then by substituting all the notation we obtain\n",
      "\n",
      "$$ \\mathbb{E}(X-  \\mathbb{E}(X|Y))^2 = \\mathbb{E}(X)^2 - \\mathbb{E}(\\mathbb{E}(X|Y) )^2    $$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's consider the following example with $f_{X,Y}= x + y $ where $(x,y) \\in [0,1]^2$ and we'll compute the conditional expectation straight from the definition:\n",
      "\n",
      "$$ \\mathbb{ E}(X|Y) = \\int_0^1 x \\frac{f_{X,Y}(x,y)}{f_Y(y)} dx=  \\int_0^1 x \\frac{x+y}{y+1/2} dx =\\frac{3 y + 2}{6 y + 3} $$\n",
      "\n",
      "That was pretty easy because the density function is so simple. Now, let's do it the hard way by going directly for the MMSE solution $h(Y)$. Then,\n",
      "\n",
      "$$ \\min_h \\int_0^1\\int_0^1 (x - h(y) )^2 f_{X,Y}(x,y)dx dy = \\min_h \\int_0^1  y h^2 {\\left (y \\right )} - y h{\\left (y \\right )} + \\frac{1}{3} y + \\frac{1}{2} h^{2}{\\left (y \\right )} - \\frac{2}{3} h{\\left (y \\right )} + \\frac{1}{4} dy $$ \n",
      "\n",
      "Now we have to find a function $h$ that is going to minimize this. Solving for a function, as opposed to solving for a number, is generally very, very hard, but in this case we can use the Euler-Lagrange method from variational calculus to take the derivative of the integrand with respect to the function $h(y)$ and set it to zero. Euler-Lagrange methods will be the topic of a later section, but for now we just want the result, namely,\n",
      "\n",
      "$$ 2 y h{\\left (y \\right )} - y + h{\\left (y \\right )} - \\frac{2}{3} =0 $$\n",
      "\n",
      "Solving this gives\n",
      "\n",
      "$$ h(y)= \\frac{3 y + 2}{6 y + 3} $$\n",
      "\n",
      "Thus, doing it both ways gives us the same answer; but, in general, neither way is necessarily easier because they both involve potentially difficult or impossible integration or solving for functions. \n",
      "\n",
      "Before we leave this example, let's check the length of the error function using `sympy`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sympy.abc import y,x\n",
      "from sympy import integrate, simplify\n",
      "\n",
      "fxy = x + y                 # joint density\n",
      "fy = integrate(fxy,(x,0,1)) # marginal density\n",
      "fx = integrate(fxy,(y,0,1)) # marginal density\n",
      "\n",
      "h = (3*y+2)/(6*y+3) # conditional expectation\n",
      "LHS=integrate((x - h)**2 *fxy, (x,0,1),(y,0,1)) # from the definition\n",
      "RHS=integrate( (x)**2 *fx, (x,0,1)) - integrate( (h)**2 *fy, (y,0,1)) # using Pythagorean theorem\n",
      "print simplify(LHS-RHS)==0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Zero-Mean Bivariate Gaussian Case"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Conditional Expectation Using Direct Integration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's  compute $ \\mathbb{E}(X|Y)$ in the case of the bivariate Gaussian distribution straight from the definition.\n",
      "\n",
      "$$ \\mathbb{E}(X|Y)  = \\int_{\\mathbb{ R}} x \\frac{f_{X,Y}(x,y)}{f_Y(y)} dx $$ \n",
      "\n",
      "where \n",
      "\n",
      "$$ f_{X,Y}(x,y) = \\frac{1}{2\\pi |\\mathbf{R}|^{\\frac{1}{2}}} e^{-\\frac{1}{2} \\mathbf{v}^T \\mathbf{R}^{-1} \\mathbf{v} } $$ \n",
      "\n",
      "and where\n",
      "\n",
      "$$ \\mathbf{v}= \\left[ x,y \\right]^T$$ \n",
      "\n",
      "$$ \\mathbf{R} = \\left[ \\begin{array}{cc}\n",
      "\\sigma_{x}^2 & \\sigma_{xy} \\\\\n",
      "\\sigma_{xy}  & \\sigma_{y}^2 \\\\\n",
      "\\end{array} \\right] $$ \n",
      "\n",
      "and with\n",
      "\n",
      "\\begin{eqnarray}\n",
      " \\sigma_{xy} &=& \\mathbb{E}(xy)   \\nonumber    \\\\\n",
      " \\sigma_{x}^2 &=& \\mathbb{E}(x^2) \\nonumber \\\\ \n",
      " \\sigma_{y}^2 &=& \\mathbb{E}(y^2) \\nonumber      \n",
      "\\end{eqnarray}\n",
      "\n",
      "This is a tough integral to evaluate, so we'll do it with `sympy`.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sympy import Matrix, Symbol, exp, pi, simplify, integrate \n",
      "from sympy import stats\n",
      "\n",
      "sigma_x = Symbol('sigma_x',positive=True)\n",
      "sigma_y = Symbol('sigma_y',positive=True)\n",
      "sigma_xy = Symbol('sigma_xy',real=True)\n",
      "fyy = stats.density(stats.Normal('y',0,sigma_y))(y)\n",
      " \n",
      "R = Matrix([[sigma_x**2, sigma_xy],\n",
      "            [sigma_xy,sigma_y**2]])\n",
      "fxy = 1/(2*pi)/sqrt(R.det()) * exp((-Matrix([[x,y]])*R.inv()* Matrix([[x],[y]]))[0,0]/2 )\n",
      "\n",
      "fcond = simplify(fxy/fyy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unfortunately, `sympy` cannot immediately integrate this without some hints. So, we need to define a positive variable ($u$) and substitute it into the integration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u=Symbol('u',positive=True) # define positive variable\n",
      "\n",
      "fcond2=fcond.subs(sigma_x**2*sigma_y**2-sigma_xy**2,u) # substitute as hint to integrate\n",
      "g=simplify(integrate(fcond2*x,(x,-oo,oo))) # evaluate integral\n",
      "gg=g.subs( u,sigma_x**2 *sigma_y**2 - sigma_xy**2 ) # substitute back in\n",
      "use( gg, simplify,level=2) # simplify exponent term"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "$$\\frac{\\sigma_{xy} y}{\\sigma_{y}^{2}}$$"
       ],
       "output_type": "pyout",
       "png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAApCAYAAAAmukmKAAAABHNCSVQICAgIfAhkiAAAAipJREFU\nWIXt109sTUEUx/FPX1sSQi2IIP5VUn9qY0HQSEmQ2BASCzZE7FhoSGz820iasCUhLC1F6EKaiqQ7\nFspKkDQhJQghJf5rWczc9LX1+t59va8W7Xczd2bOzO+8mXfPOZf/zBE8xHd8xRPcTLG+GgdwBY1x\nbCYeJAa5PONWzEEz5uEVlmNHCsGdaMNUrIhjm9Az1HAp7g9x4DHqUoiJ9nV4F0XhAloSg0RgM9rR\nH/uz8Bu9KQV7sQ2d+BLHNsb+IMH3+Ja38ByOpRRLmIvu+Lwk9h8lk9WxfYqtaBDurA0dcW6GcAK7\nsCw60iHc0T78Eo7wMm7hLfYIp3U49q+l8XhxbE9gtYF7XSjcz3zh2I7/Y20H9qcRg+moEbzPYVHe\n3I3Yno3ONOI5qtCELtTmb1atOKdQHxf+iGNvYrsOn3EUp+PYbCzAWhwU3udMacDFUo2rRiG0W3h9\npuCS8EsnGAckf5o/Yy1YDjkhuvSjD+cz8WgEtgtpDa4LcbYoueImBak3kJy7hQBQUSZjWnxuF9JQ\nUUqJpYXow09sEO7x9ij2Kpk6nBwLoYRDQhapFZJ0RdmLT0J58hErs9h0tHXqMGpGmGsVAkOzcGT3\nhDq1IlEpqzp1GIVe/Kzq1JIFs6xTB1EoeNfgDF4K3xmduJOFYDmsErLEGkzC1TSLywneH4Qy8DXW\n41mlBV8IgboHW3C30oKJaJOQE7vK3CM1qQrghHJKjIkCeJzzF8g2ahW/sT5UAAAAAElFTkSuQmCC\n",
       "prompt_number": 7,
       "text": [
        "sigma_xy\u22c5y\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "        2 \n",
        " sigma_y  "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus, by direct integration using `sympy`, we found\n",
      "\n",
      "$$ \\mathbb{ E}(X|Y) = y \\frac{\\sigma_{xy}}{\\sigma_{y}^{2}} $$ "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Conditional expectation by Optimization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's try that again by finding a  function $h$ that minimizes the MSE.\n",
      "\n",
      "As we said, the trouble is that trying to solve for the conditional expectation by minimizing the error over all possible functions $h$ is generally very, very hard. One alternative is to use parameters for the $h$ function and then just optimize over those. For example, we could assume that $h(Y)= \\alpha Y$ and then use calculus to find the $\\alpha$.\n",
      "\n",
      "Let's try this with the zero-mean bivariate Gaussian density,\n",
      "\n",
      "$$\\mathbb{E}((X-\\alpha Y )^2) = \\mathbb{E}(\\alpha^2 Y^2 - 2 \\alpha X Y + X^2 )$$\n",
      "\n",
      "and then differentiate this with respect to $\\alpha$ to obtain\n",
      "\n",
      "$$\\mathbb{E}(2 \\alpha Y^2 - 2 X Y  ) = 2 \\alpha \\sigma_y^2 - 2 \\mathbb{E}(XY) = 0$$\n",
      "\n",
      "Then, solving for $\\alpha$ gives\n",
      "\n",
      "$$ \\alpha = \\frac{ \\mathbb{E}(X Y)}{ \\sigma_y^2 } $$\n",
      "\n",
      "which means we claim that\n",
      "\n",
      "$$ \\mathbb{ E}(X|Y) = \\alpha Y =   \\frac{ \\mathbb{E}(X Y )}{ \\sigma_Y^2 } Y =\\frac{\\sigma_{X Y}}{ \\sigma_Y^2 } Y  $$\n",
      "\n",
      "where that last step is just notation.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ \\min_h \\int_\\mathbb{R^2} | X - h(Y) |^2 f_{x,y}(X,Y) dx dy $$\n",
      "\n",
      "$$ \\min_h \\int_\\mathbb{R^2} | X |^2 f_{x,y}(X,Y) dx dy + \\int_\\mathbb{R^2} | h(Y) |^2 f_{x,y}(X,Y) dx dy - \\int_\\mathbb{R^2} 2 X h(Y) f_{x,y}(X,Y) dx dy $$\n",
      "\n",
      "Now, we want to maximize the following:\n",
      "\n",
      "$$ \\max_h \\int_\\mathbb{R^2}  X h(Y) f_{x,y}(X,Y) dx dy  $$ \n",
      "\n",
      "Breaking up the integral using the definition of conditional expectation\n",
      "\n",
      "$$ \\max_h \\int_\\mathbb{R}   \\left(\\int_\\mathbb{R} X  f_{x|y}(X|Y) dx \\right)h(Y) f_Y(Y)   dy  $$ \n",
      "\n",
      "$$ \\max_h \\int_\\mathbb{R} \\mathbb{E}(X|Y) h(Y)f_Y(Y)   dy  $$ \n",
      "\n",
      "From properties of the Cauchy-Schwarz inequality, we know that the maximum happens when $h(Y) = \\mathbb{E}(X|Y)$, so we have found the optimal $h(Y)$ function as :\n",
      "\n",
      "$$ h(Y) = \\mathbb{E}(X|Y)$$ \n",
      "\n",
      "which shows that the optimal function is the conditional expectation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we have a random variable, $X$, then what constant is closest to $X$ in the mean-squared-sense (MSE)? In other words, which $c$ minimizes the following:\n",
      "\n",
      "$$ J = \\mathbb{E}( X - c )^2 $$ \n",
      "\n",
      "we can work this out as\n",
      "\n",
      "$$ \\mathbb{E}( X - c )^2 = \\mathbb{E}(c^2 - 2 c X + X^2)  $$ \n",
      "\n",
      "and then take the first derivative with respect to $c$ and solve:\n",
      "\n",
      "$$ c=\\mathbb{E}(X) $$ \n",
      "\n",
      "Remember that $X$ can take on all kinds of values, but this says that the closest number to $X$ in the MSE sense is $\\mathbb{E}(X)$.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}