{
 "metadata": {
  "name": "Conditional_Expectation_Projection"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "in our discussion, we previously noted that the conditional expectation is the minimum mean squared error (MMSE)solution to the following problem:\n",
      "\n",
      "$$ \\min_h \\int_{\\mathbb{R}} (X - h(Y) )^2 dX $$ \n",
      "\n",
      "$$ \\mathbb{E}(X|Y) = h_{opt}(Y) $$\n",
      "\n",
      "which is another way of saying that among all possible functions of $h(Y)$, the one that minimizes the MSE is $ \\mathbb{E}(X|Y)$. From our discussion on projection, we noted that these MMSE solutions can be thought of as projections onto a subspace that characterizes $Y$ in some sense. The amazing thing is that we can shift our notion of the inner product from our last discussion on projections and collect all of the same results (with some additional technicalities). For example, we previously noted that at the point of projection, we had\n",
      "\n",
      "$$ ( \\mathbf{y} - \\alpha \\mathbf{v} )^T \\mathbf{v} = 0$$\n",
      "\n",
      "which by noting the inner product as $\\langle\\mathbf{x},\\mathbf{y} \\rangle = \\mathbf{x}^T \\mathbf{y}$, we can express as\n",
      "\n",
      "$$\\langle \\mathbf{y} - \\alpha \\mathbf{v},\\mathbf{v} \\rangle = 0  $$ \n",
      "\n",
      "and, in fact, by defining the inner product for the random variables $X$ and $Y$ as \n",
      "\n",
      "$$ \\langle X,Y \\rangle = \\mathbb{E}(X Y)$$ \n",
      "\n",
      "we have the same relationship,\n",
      "\n",
      "$$\\langle X-h_{opt}(Y),Y \\rangle = 0 =  \\mathbb{E}( X-\\mathbb{E}(X|Y), Y) $$ \n",
      "\n",
      "and using the linearity of the expectation, we can easily obtain,\n",
      "\n",
      "$$  \\mathbb{E}(X Y) =  \\mathbb{E}(Y \\mathbb{E}(X|Y))$$  \n",
      "\n",
      "If that seems too easy, we can proceed with a formal definition of conditional expectation\n",
      "\n",
      "$$ \\mathbb{E}(X|Y) = \\int_{\\mathbb{R}^2} x \\frac{f_{X,Y}(x,y)}{f_Y(y)} dx dy  $$\n",
      "\n",
      "and show this formally,\n",
      "\n",
      "$$ \\mathbb{E}(Y \\mathbb{E}(X|Y))= \\int_{\\mathbb{R}} y \\int_{\\mathbb{R}} x \\frac{f_{X,Y}(x,y)}{f_Y(y)}  f_Y(y) dx dy $$\n",
      "\n",
      "and we can cancel the $f_Y$ and obtain\n",
      "\n",
      "$$ \\mathbb{E}(Y \\mathbb{E}(X|Y))=\\int_{\\mathbb{R}^2} x y f_{X,Y}(x,y) dx dy =\\mathbb{E}( X Y) $$\n",
      "\n",
      "But doing it formally prohibits us from carrying along the geometric  interpretation from our previous discussion.\n",
      "\n",
      "We can keep pursuing this as before and obtain the length of the error term as\n",
      "\n",
      "$$ \\langle X-h_{opt}(Y),X-h_{opt}(Y)  \\rangle = \\langle X,X  \\rangle - \\langle h_{opt}(Y),h_{opt}(Y)  \\rangle  $$\n",
      "\n",
      "and then by substituting all the notation we obtain\n",
      "\n",
      "$$ \\mathbb{E}(X-  \\mathbb{E}(X|Y))^2 = \\mathbb{E}(X)^2 - \\mathbb{E}(\\mathbb{E}(X|Y) )^2    $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Proof that $\\mathbb{E}(x|y)$ is Minimum MSE solution\n",
      "\n",
      "$$ $$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ \\min_h \\int_\\mathbb{R^2} | X - h(Y) |^2 f_{x,y}(X,Y) dx dy $$\n",
      "\n",
      "$$ \\min_h \\int_\\mathbb{R^2} | X |^2 f_{x,y}(X,Y) dx dy + \\int_\\mathbb{R^2} | h(Y) |^2 f_{x,y}(X,Y) dx dy - \\int_\\mathbb{R^2} 2 X h(Y) f_{x,y}(X,Y) dx dy $$\n",
      "\n",
      "Now, we want to maximize the following:\n",
      "\n",
      "$$ \\max_h \\int_\\mathbb{R^2}  X h(Y) f_{x,y}(X,Y) dx dy  $$ \n",
      "\n",
      "Breaking up the integral using the definition of conditional expectation\n",
      "\n",
      "$$ \\max_h \\int_\\mathbb{R}   \\left(\\int_\\mathbb{R} X  f_{x|y}(X|Y) dx \\right)h(Y) f_Y(Y)   dy  $$ \n",
      "\n",
      "$$ \\max_h \\int_\\mathbb{R} \\mathbb{E}(X|Y) h(Y)f_Y(Y)   dy  $$ \n",
      "\n",
      "From properties of the Cauchy-Schwarz inequality, we know that the maximum happens when $h(Y) = \\mathbb{E}(X|Y)$, so we have found the optimal $h(Y)$ function as :\n",
      "\n",
      "$$ h(Y) = \\mathbb{E}(X|Y)$$ \n",
      "\n",
      "which shows that the optimal function is the conditional expectation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we have a random variable, $X$, then what constant is closest to $X$ in the mean-squared-sense (MSE)? In other words, which $c$ minimizes the following:\n",
      "\n",
      "$$ J = \\mathbb{E}( X - c )^2 $$ \n",
      "\n",
      "we can work this out as\n",
      "\n",
      "$$ \\mathbb{E}( X - c )^2 = \\mathbb{E}(c^2 - 2 c X + X^2)  $$ \n",
      "\n",
      "and then take the first derivative with respect to $c$ and solve:\n",
      "\n",
      "$$ c=\\mathbb{E}(X) $$ \n",
      "\n",
      "Remember that $X$ can take on all kinds of values, but this says that the closest number to $X$ in the MSE sense is $\\mathbb{E}(X)$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's continue our discussion of the conditional expectation operator by connecting it to the concept of projection. \n",
      "\n",
      "In the figure below, I want to find a point along the blue line that is closest to the red square. In other words, I want to inflate the pink circle until it just touches the blue line. Then, that point will be the closest point on the blue line to the red square."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It may be geometrically obvious, but the closest point on the line occurs where the line from the red square to the line is perpedicular to the line. At this point, the pink circle just touches the blue line."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is an arbitrary point along the blue line:\n",
      "\n",
      "$$ \\mathbf{x} = \\alpha \\mathbf{v} $$ \n",
      "\n",
      "where \n",
      "\n",
      "$$ \\mathbf{v} = \\left[ \\begin{array}{c}\n",
      "1 \\\\\n",
      "1 \\\\\n",
      "\\end{array} \\right] $$ \n",
      "\n",
      "so that $\\alpha$ slides the point up and down the line. At the closest point, the vector between $\\mathbf{y}$ and $\\mathbf{x}$ (the dotted green line above) is perpedicular to the line. This means that\n",
      "\n",
      "$$ ( \\mathbf{y}-\\mathbf{x} )^T \\mathbf{v} = 0$$ \n",
      "\n",
      "and by substituting and working out the terms gives \n",
      "\n",
      "$$ \\alpha = \\frac{\\mathbf{y}^T\\mathbf{v}}{|\\mathbf{v}|^2}  = \\frac{1}{2} ( y_0 + y_1 )$$\n",
      "\n",
      "Using the Pythagorean theorem, we compute the squared length of the error as\n",
      "\n",
      "$$ \\epsilon^2 = |( \\mathbf{y}-\\mathbf{x} )|^2 = |\\mathbf{y}|^2 - \\alpha^2 |\\mathbf{v}|^2 = |\\mathbf{y}|^2 - \\frac{|\\mathbf{y}^T\\mathbf{v}|^2}{|\\mathbf{v}|^2}  $$\n",
      "\n",
      "where $ |\\mathbf{v}|^2 = \\mathbf{v}^T \\mathbf{v} $. Note that since $\\epsilon^2 \\ge 0 $, this also shows that\n",
      "\n",
      "$$ |\\mathbf{y}^T\\mathbf{v}| \\le |\\mathbf{y}|  |\\mathbf{v}|   $$ \n",
      "\n",
      "which is the famous Cauchy-Schwarz inequality. This inequality will soon be very important  for us.\n",
      "\n",
      "Finally, we can assemble all of this into the *projection* operator\n",
      "\n",
      "$$ \\mathbf{P}_v = \\frac{1}{|\\mathbf{v}|^2 } \\mathbf{v v}^T $$\n",
      "\n",
      "With this operator, we can take any given $\\mathbf{y}$ and project it onto $\\mathbf{v}$ by doing\n",
      "\n",
      "$$ \\mathbf{P}_v \\mathbf{y} $$ \n",
      "\n",
      "It's called an operator because it takes a vector and produces another vector.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}