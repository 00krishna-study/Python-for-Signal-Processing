{
 "metadata": {
  "name": "Sampling Theorem"
 },
 "nbformat": 3,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "source": [
      "Sampling Theorem"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "In this section, we investigate the implications of the sampling theorem. Here is the usual statement of the theorem from wikipedia:",
      "",
      "*\"If a function $x(t)$ contains no frequencies higher than B hertz, it is completely determined by giving its ordinates at a series of points spaced 1/(2B) seconds apart.\"*",
      "",
      "Since a function $x(t)$ is a function from the real line to the real line, there are uncountably many points between any two ordinates, so sampling is a massive reduction of data since it only takes a tiny number of points to completely characterize the function. This is a powerful idea worth exploring. In fact, we have seen this idea of reducing a function to a discrete set of numbers before in Fourier series expansions where (for periodic $x(t)$ ) ",
      "",
      "<center>",
      "$ a_n = \\frac{1}{T} \\int^\\infty_{-\\infty} x(t) exp (-j \\omega t )dt $",
      "</center>    ",
      "    ",
      "",
      "with corresponding reconstruction as:",
      "",
      "<center>",
      "$ x(t) = \\Sigma_k a_n exp( j \\omega t n) $",
      "</center>",
      "",
      "But here we are generating discrete points $a_n$ by integrating over the **entire** function $x(t)$, not just evaluating it at a single point. This means we are collecting information about the entire function to compute a single discrete point $a_n$, whereas with sampling we are just taking individual points in isolation.",
      "",
      "Let's come at this the other way: suppose we are given a set of samples $[x_1,x_2,..,x_N]$ and we are then told to reconstruct the function. What would we do? This is the kind of question seldom asked because we typically sample, filter, and then do something else without trying to reconstruct the function from the samples directly.",
      "",
      "Returning to our reconstruction challenge, perhaps the most natural thing to do is draw a straight line between each of the points as in linear interpolation. The next block of code takes samples of the $sin$ over a single period and draws a line between sampled ordinates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib.pylab import * # this imports numpy too",
      "close('all')",
      "f = 1.0  # Hz, signal frequency",
      "fs = 5.0 # Hz, sampling rate (ie. >= 2*f) ",
      "t = arange(-1,1+1/fs,1/fs) # sample interval, symmetric for convenience later",
      "x = sin(2*pi*f*t)",
      "plot(t,x,'o-')",
      "show()"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "source": [
      "In this plot, notice how near the extremes of the $sin$ at $t=1/(4f)$ and $t=3/(4 f)$, we are taking the same density of points since the sampling theorem makes no requirement on *where* we should sample as long as we sample at a regular intervals. This means that on the up and down slopes of the $sin$, which are obviously linear-looking and where a linear approximation is a good one, we are taking the same density of samples as near the curvy peaks. Here's a bit of code that zooms in to the first peak to illustrate this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(t,x,'o-')",
      "axis( xmin = 1/(4*f)-1/fs*3, xmax = 1/(4*f)+1/fs*3, ymin = 0, ymax = 1.1 )",
      "show()"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "source": [
      "To drive this point home (and create some cool matplotlib plots), we can construct the piecewise linear interpolant and compare the quality of the approximation using ``numpy.piecewise``:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interval=[] # piecewise domains",
      "apprx = []  # line on domains",
      "# build up points *evenly* inside of intervals",
      "tp = hstack([ linspace(t[i],t[i+1],20,False) for i in range(len(t)-1) ])",
      "# construct arguments for piecewise2",
      "for i in range(len(t)-1):",
      "   interval.append( np.logical_and(t[i] <= tp,tp < t[i+1]))",
      "   apprx.append( (x[i+1]-x[i])/(t[i+1]-t[i])*(tp[interval[-1]]-t[i]) + x[i])",
      "x_hat = np.piecewise(tp,interval,apprx) # piecewise linear approximation"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "source": [
      "Now, we can examine the squared errors in the interpolant. The following snippet plots the $sin$ and with the filled-in error of the linear interpolant."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ax1 = figure().add_subplot(111)",
      "ax1.fill_between(tp,x_hat,sin(2*pi*f*tp),facecolor='red')",
      "ax2 = ax1.twinx()",
      "sqe = ( x_hat - sin(2*pi*f*tp))**2",
      "ax2.plot(tp, sqe,'r')",
      "ax2.axis(xmin=-1,ymax= sqe.max() )",
      "ax2.set_ylabel('squared error', color='r')",
      "ax1.set_title('Errors with Piecewise Linear Interpolant')",
      "show()"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "source": [
      "Note: I urge you to change the $fs$ sampling rate in the code above then rerun this notebook to see how these errors change with more/less sampling points."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "Now, we could pursue this line of reasoning with higher-order polynomials instead of just straight lines, but this would all eventually take us to the same conclusion; namely, that all of these approximations improve as the density of sample points increases, which is the *exact* opposite of what the sampling theorem says --- there is *sparse* set of samples points that will retrieve the original function. Furthermore, we observed that the quality of the piecewise linear interpolation is sensitive to *where* the sample points are taken and the sampling theorem is so powerful that it *has no such requirement*. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "source": [
      "Reconstruction"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "Let's look at this another way by examing the Fourier Transform of a signal that is bandlimited and thus certainly satisfies the hypothesis of the sampling theorem:",
      "",
      "<center>",
      "    $ X(f) = 0$ where $ |f|> W $",
      "</center>",
      "",
      "Now, the inverse Fourier transform of this is the following:",
      "    ",
      "<center>    ",
      "    $ x(t) = \\int_{-W}^W X(f) e^{j 2 \\pi f t} df $",
      "</center>",
      "",
      "We can take the $X(f)$ and expand it into a Fourier series by pretending that it is periodic with period $2 W$. Thus, we can formally write the following:",
      "    ",
      "<center>    ",
      "    $ X(f) = \\sum_k a_k e^{ - j 2 \\pi k f/(2 W) }  $",
      "</center>",
      "we can compute the coefficients $a_k$ as ",
      "<center>    ",
      "    $ a_k = \\frac{1}{2 W} \\int_{-W}^W  e^{ j 2 \\pi k f/(2 W) } df   $",
      "</center>    ",
      "",
      "These coefficients bear a striking similarity to the $x(t)$ integral we just computed above. In fact, by lining up terms, we can write:",
      "<center>    ",
      "    $ a_k = \\frac{1}{2 W} x( t = \\frac{k}{2 W} )  $",
      "</center>        ",
      "",
      "Now, we can write out $X(f)$ in terms of this series and these $a_k$ and then invert the Fourier transform to obtain the following:",
      "",
      "$ x(t) = \\int_{-W}^W \\sum_k a_k e^{ - j 2 \\pi k f/(2 W) }  e^{j 2 \\pi f t} df $",
      "",
      "substitute for $a_k$",
      "    ",
      "$ x(t) = \\int_{-W}^W \\sum_k ( \\frac{1}{2 W} x( t = \\frac{k}{2 W} ) ) e^{ - j 2 \\pi k f/(2 W) }  e^{j 2 \\pi f t} df $",
      "    ",
      "switch summation and integration (usually dangerous, but OK here)",
      "    ",
      "$ x(t) = \\sum_k x(t = \\frac{k}{2 W}) \\frac{1}{2 W}  \\int_{-W}^W e^{ - j 2 \\pi k f/(2 W) +j 2 \\pi f t} df  $",
      "    ",
      "which gives finally:",
      "",
      "<center>",
      "$ x(t) = \\sum_k x(t = \\frac{k}{2 W})  \\frac{sin(\\pi (k-2 t W))} {\\pi (k- 2 t W)} $            ",
      "</center>",
      "",
      "And this what we have been seeking! A formula that reconstructs the function from it's samples. Let's try it!"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "Note that since our samples are spaced at $t= k/f_s $, we'll use $  W= f_s /2 $ to line things up."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = linspace(-1,1,100) # redefine this here for convenience",
      "ts = arange(-1,1+1/fs,1/fs) # sample points",
      "num_coeffs=len(ts) ",
      "sm=0",
      "for k in range(-num_coeffs,num_coeffs): # since function is real, need both sides",
      "   sm+=sin(2*pi*(k/fs))*sinc( k - fs * t)",
      "close('all')",
      "plot( t,sm,'--',t,sin(2*pi*t),ts, sin(2*pi*ts),'o')",
      "title('sampling rate=%3.2f Hz' % fs )",
      "show()"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "source": [
      "We can do the same check as we did for the linear interpolant above as"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ax1 = figure().add_subplot(111)",
      "ax1.fill_between(t,sm,sin(2*pi*f*t),facecolor='red')",
      "ax2 = ax1.twinx()",
      "sqe = (sm - sin(2*pi*f*t))**2",
      "ax2.plot(t, sqe,'r')",
      "ax2.axis(xmin=0,ymax = sqe.max())",
      "ax2.set_ylabel('squared error', color='r')",
      "ax1.set_title('Errors with sinc Interpolant')",
      "show()"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "source": [
      "These interpolating functions are called the \"Whittaker\" interpolating functions. Let's examine these functions more closely with the following code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "figure()",
      "k=0",
      "plot (t,sinc( k - fs * t), t,sinc( k+1 - fs * t),k/fs,1,'o',(k)/fs,0,'o')",
      "hlines(0,-1,1)",
      "vlines(0,-.2,1)",
      "show()"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "source": [
      "The vertical line in the previous plot shows that where one function has a peak, the other function has a zero. This is why when you put samples at each of the peaks, they match the sampled function exactly at those points. In between those points, the crown shape of the functions fills in the missing values. Thus, the sampling theorem says that the filled-in values are drawn from the curvature of the sinc functions, not straight lines as we investigated earlier. ",
      "",
      "It turns out that one of the key properties for interpolating functions is that they have to somehow \"resolve the identity\" or sum to the number one. Otherwise, you couldn't get constants in your interpolation and that would adversely restrict the types of functions one could successfully approximate. Let's examine how well we can resolve the identity with the following bit of code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "figure()",
      "num_coefs = 10 # change this and examine the resulting plots",
      "plot (t,(reduce( lambda i,k:i+sinc(k-fs*t),range(-num_coefs,num_coefs),0)-1)**2)",
      "show()"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "source": [
      "The problem with the situation is that we really need a *lot* of terms in order to get the ripple down enought to satisfy some given accuracy. In the next post, we'll examine the consequences of this behavior as we try to figure out how many samples we need even if we satisfy the requirements of the sampling theorem."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "source": [
      "Uncertainty Principle for Signals"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "Recall the scaling theorem from Fourier analysis:",
      "",
      "$$ a f( a t) \\leftrightarrow F(\\omega/a) $$",
      "",
      "This basically says that if you expand the time-scale by the factor $a$, then this causes a corresponding compression in the frequency scale by $1/a$. The sampling theorem makes a statement about the bandlimited nature of $F(\\omega)$ so let's investigate what this means in terms of the energy concentration in the time and frequency domains using the following definitions:",
      "",
      "$$ \\alpha = \\frac{1}{E} \\int_{-\\tau}^\\tau \\vert f(t)\\vert ^2 dt $$",
      "$$ \\beta = \\frac{1}{2 \\pi E} \\int_{-\\sigma}^\\sigma \\vert F(\\omega)\\vert ^2 d\\omega $$",
      "",
      "where $\\tau$ measures the extent of the signal in time and $\\sigma$ measures the extent of the signal in frequency. Also, $E$ is the total energy in the signal:",
      "",
      "$$ E = \\int^\\infty_{-\\infty} \\vert f(t)\\vert ^2 dt = \\int^\\infty_{-\\infty} \\vert F(\\omega)\\vert ^2 d\\omega $$",
      "",
      "In terms of the sampling theorem, this says that the bandwidth $B=2 \\sigma$. With these definitions, we are investigating what happened when we \"chop\" the signal off in both time and frequency. I'm not going to go through the proof of the following result here, you can find it in chapter 8 of Papoulis' \"Signal Analysis\", but the main idea is the following:",
      "",
      "$$ \\tau \\sigma \\ge c $$",
      "",
      "The product on the left is called the *time-bandwidth* product and the right is a constant that depends on the solution of an integral equation. ",
      "",
      "In other words, given a signal with a fixed bandwidth, $2 \\sigma$, we need to sample the signal for at least a duration $ 2\\tau $. This is the dirty little secret that is missing from the statement of the sampling theorem, which implicitly assumes that signal lasts forever in the time-domain. Obviously, since we want to compute this in a computer with finite memory, we cannot manage an infinite number of samples and this result tells us how low we can go at the given sample rate in order to completely recover the signal.",
      "",
      "This result says something even more important: you cannot independently limit the signal in time and frequency simultaneously! This is because, for example, limiting in time is the same as expanding in frequency (via the scaling theorem).  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      ""
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": "*"
    }
   ]
  }
 ]
}